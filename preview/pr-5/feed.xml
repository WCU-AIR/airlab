<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/airlab/preview/pr-5/feed.xml" rel="self" type="application/atom+xml" /><link href="/airlab/preview/pr-5/" rel="alternate" type="text/html" /><updated>2025-10-09T03:17:52+00:00</updated><id>/airlab/preview/pr-5/feed.xml</id><title type="html">WCU-AIR</title><subtitle>The Artificial Intelligence Research (AIR) Lab at West Chester University is a collaborative community of faculty and students exploring how AI can make a positive impact. We bring together diverse computer science, engineering, and education perspectives, with a strong emphasis on mentorship and hands-on experience. Our members contribute to publications, open-source projects, and applied collaborations while developing skills that prepare them for both academic and industry careers. By fostering inclusion and curiosity, the AIR Lab provides an environment where innovation and learning thrive side by side.</subtitle><entry><title type="html">Memory Optimization Demo</title><link href="/airlab/preview/pr-5/2025/10/01/memory-optimizations.html" rel="alternate" type="text/html" title="Memory Optimization Demo" /><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-09T03:14:23+00:00</updated><id>/airlab/preview/pr-5/2025/10/01/memory-optimizations</id><content type="html" xml:base="/airlab/preview/pr-5/2025/10/01/memory-optimizations.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In my <a href="/airlab/preview/pr-5/blog/_posts/2025-09-20-computing-resources.html" title="Computing Resources Blog">last blog post</a>, we looked at some past and projected figures for AI’s natural-resource consumption and showed the expenditure to be immense. Following that, we gave a high-level view of current or proposed methods to mitigate those figures. In this blog post, we’ll inspect a couple current software-level optimizations (kernel fusion and gradient checkpointing) and implement some simple demos to deepen our understanding of how they work. Following that, I’ll give a brief reflection on what I’ve gained from writing this post and what I plan to do next.</p>

<h2 id="background">Background</h2>
<!-- excerpt start -->
<p>To pragmatically address the bottleneck of input/output operations, let’s look at a couple specific ways we can better utilize the computing resources we have available: kernel fusion and gradient checkpointing<!-- excerpt end --><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Before getting into their implementations, we’ll review each one conceptually.</p>

<h3 id="kernel-fusion">Kernel Fusion</h3>
<p>Since i/o operations to secondary memory are more computationally expensive, kernel fusion aims to reduce them by maximizing the potential of our primary memory. Before going any further, we should clarify what a kernel is. We can think of it as a function that is carried out by number of threads in parallel on a GPU. Somewhat analogous to combining multiple subprocesses on a CPU, this technique combines multiple operations into a single environment or context, instead of executing them sequentially and reading/writing temporary values to secondary memory after each one, since they do not share primary memory. Additionally, just as spawning subprocess involves overhead components, so too does a kernel, meaning fusion reduces these costs as well.</p>

<h3 id="gradient-checkpointing">Gradient Checkpointing</h3>
<p>Since bottlenecks are often caused by memory operations and leave computational units idle, we can use the gradient checkpointing strategy to reduce the amount of memory we use in return for increasing the amount of computations being done. In other words, instead of recording all of the data you need, record only a subset (checkpoints) and use it to redo the rest of the calculations as needed. It’s like the reverse of memoization, in which a program avoids recomputing values by saving them for reuse.</p>

<h2 id="demos">Demos</h2>

<h3 id="user-driven-kernel-fusion">User-Driven Kernel Fusion</h3>
<p>To simplify this process, consider the following conceptual example:</p>

<p>$$(5 + 3) * 10$$</p>

<p>To evaluate this expression, we could do the following</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add_and_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">temp</span> <span class="o">*</span> <span class="n">c</span>
<span class="k">print</span><span class="p">(</span><span class="n">add_and_multiply</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p>In this example, we create a temporary variable to represent the intermediary result. Instead of allocating this memory to add and multiply separately, we can combine them into a single operation, removing the need to allocate memory and read/write to it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add_and_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
<span class="k">print</span><span class="p">(</span><span class="n">add_and_multiply</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p>Comparing these two functions is a very simple way of illustrating what happens when kernels are fused in machine-learning tasks. While this example is small, the classic example for kernel fusion involves linear algebra and sequential matrix operations. Data for these operations can measure in gigabytes and utilize secondary memory.</p>

<h4 id="compiler-driven-kernel-fusion">Compiler-Driven Kernel Fusion</h4>
<p>While a developer can manually combine operations, this strategy is largely applied at a compiler level. Instead of applying optimizations directly to source code, IRs (intermediate representations) enable the transformation of high-level languages into a lower, hardware-agnostic form, while still retaining important semantic information from the original form. MLIR (Multi-Level Intermediate Representation) is a specific form or framework which makes use of fusion and is widely used for machine learning. Like the name suggests, MLIR allows code to be represented at varying levels of abstraction, each conducive to different optimizations<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<p>Using MLIR, compilers can view code as a DAG (directed acyclic graph) which shows data dependencies within the program. With nodes in the graph representing operators, it reveals the flow of data from one operator to the next. When an operator immediately consumes the output from the one preceding it, it could be advantageous to fuse the operations rather than storing and subsequently collecting the data. Let’s take a look at a simple arithmetic example. Consider the expression:</p>

<p>$$a + b * c + 1$$</p>

<p>To keep things more accessible, let’s implement our own tiny IR just for this example. The signatures could look like this:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">Node</span> <span class="p">{</span>
    <span class="kr">inline</span> <span class="k">static</span> <span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span><span class="p">;</span>
    <span class="n">Node</span><span class="p">()</span> <span class="o">:</span> <span class="n">id</span><span class="p">(</span><span class="n">counter</span><span class="o">++</span><span class="p">)</span> <span class="p">{}</span>
    <span class="k">virtual</span> <span class="o">~</span><span class="n">Node</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarA</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarA</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarB</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarB</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarC</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarC</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Const</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">value</span><span class="p">;</span>
    <span class="n">Const</span><span class="p">(</span><span class="kt">double</span> <span class="n">v</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Add</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">;</span>
    <span class="n">Add</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">l</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Mult</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">;</span>
    <span class="n">Mult</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">l</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">FusedOp</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">;</span>
    <span class="k">explicit</span> <span class="n">FusedOp</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&gt;</span> <span class="n">in</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div></div>

<p>If we parse this expression into a DAG using our minimal IR, we could represent it like this:</p>

<p><img src="/airlab/preview/pr-5/images/other/blogs/computing-resources/before.png" alt="Before Fusion" title="Before Fusion" /></p>

<p>From here, we can see the relationships between each term and apply a function to recursively traverse the tree. That could transform the original graph into the following:</p>

<p><img src="/airlab/preview/pr-5/images/other/blogs/computing-resources/after.png" alt="After Fusion" title="After Fusion" /></p>

<p>Implementing this practice in a compiler removes the need to manually apply it to all operations and allows for further optimizations at a smaller level.</p>

<p>The complete code for this implementation can be found <a href="https://github.com/brankominick/fusion-demo">here</a>.</p>

<h3 id="gradient-checkpointing-1">Gradient Checkpointing</h3>
<p>Unlike kernel fusion, this method is more limited in its use cases but very applicable to the backwards passes in machine learning and AI training. Because it’s more specific to AI training, it can be found implemented at the framework level like in PyTorch<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<p>In order to do a short demonstration without abstracting away the logic, let’s put together a simple implementation and take some benchmarks. To account for storage, we compare saving each intermediate value with saving only some:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span> <span class="nf">compute_with_storage</span><span class="p">(</span><span class="kt">double</span> <span class="n">x</span><span class="p">,</span> <span class="kt">int</span> <span class="n">depth</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">intermediates</span><span class="p">(</span><span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">intermediates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">depth</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"  Storage used: "</span>
              <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">fixed</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">setprecision</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
              <span class="o">&lt;&lt;</span> <span class="n">memory_in_mb</span><span class="p">(</span><span class="n">intermediates</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
              <span class="o">&lt;&lt;</span> <span class="s">" MB</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">depth</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To cut down on intermediates, instead of recording each one, we can update a variable and only record the checkpoints:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">depth</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">.</span><span class="n">back</span><span class="p">();</span>

        <span class="kt">int</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">checkpoint_interval</span><span class="p">,</span> <span class="n">depth</span> <span class="o">-</span> <span class="n">i</span><span class="p">);</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">steps</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">val</span> <span class="o">=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">val</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">checkpoints</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">steps</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>With an x value of 1.001 and a ridiculous depth of 2 billion, we get the following results:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Storage used: 15258.79 MB
Execution <span class="nb">time </span>with storage: 28.81s

K,Memory_MB,Time_s
10,1525.88,7.10
50,305.18,6.24
100,152.59,6.11
500,30.52,6.09
1000,15.26,6.10
5000,3.05,6.07
10000,1.53,6.05
50000,0.31,6.03
</code></pre></div></div>

<script src="https://d3js.org/d3.v7.min.js"></script>

<div id="chart"></div>
<div id="info-box" style="margin-top:20px; font-family:sans-serif;">
  <p>Hover over a point to see details.</p>
</div>
<script src="/airlab/preview/pr-5/assets/js/checkpoint-plot.js"></script>

<p>As we can see, adding checkpoints dramatically reduces not only the amount of memory used but also the total run time. However, the checkpoints have diminishing returns. As they become more frequent (as K decreases), the memory usage and execution time both increase. If we continue to decrease checkpoints (increase K), we would see slight increases to execution time in that direction as well.</p>

<p>The complete code for this implementation can be found <a href="https://github.com/brankominick/gradient-checkpointing-demo">here</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we deepened our understanding of kernel fusion and gradient checkpointing by implementing and visualizing them with simple demos. While each of these strategies brings its own optimizations, they also come with their own limitations, requiring careful consideration in how they’re implemented. Additionally, it’s important to remember that savings will compound when these techniques and others are applied in conjunction, resulting in higher utilization of compute units and thus, less wasted energy.</p>

<h2 id="reflection-and-future-direction">Reflection and Future Direction</h2>
<p>As noted in the conclusion, the strategies covered in this post have their limitations. Future work could include examining these bounds such as the payoff for fusion with complex kernels or how preferring primary memory may limit high-precision values. We could also explore other software-level optimization methods. Additionally, we could go in a more concrete direction and extend current frameworks like MLIR to execute and benchmark its current capabilities. Follow along to see what comes next!</p>

<h1 id="references-and-links">References and Links</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Makin, Yashasvi, and Rahul Maliakkal. “Sustainable AI Training via Hardware–Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures.” <em>arXiv</em>, preprint arXiv:2508.13163, 28 July 2025, https://arxiv.org/abs/2508.13163. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. “MLIR: Scaling compiler infrastructure for domain specific computation.” In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 2-14. IEEE, 2021. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Paszke, Adam, et al. Automatic Differentiation in PyTorch. 2017, OpenReview, https://openreview.net/pdf?id=BJJsrmfCZ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>brian-kominick</name></author><category term="ai," /><category term="memory," /><category term="resources" /><summary type="html"><![CDATA[Introduction In my last blog post, we looked at some past and projected figures for AI’s natural-resource consumption and showed the expenditure to be immense. Following that, we gave a high-level view of current or proposed methods to mitigate those figures. In this blog post, we’ll inspect a couple current software-level optimizations (kernel fusion and gradient checkpointing) and implement some simple demos to deepen our understanding of how they work. Following that, I’ll give a brief reflection on what I’ve gained from writing this post and what I plan to do next.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/airlab/preview/pr-5/images/other/blogs/computing-resources/before.png" /><media:content medium="image" url="/airlab/preview/pr-5/images/other/blogs/computing-resources/before.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Resource Efficiency and AI</title><link href="/airlab/preview/pr-5/2025/09/20/computing-resources.html" rel="alternate" type="text/html" title="Resource Efficiency and AI" /><published>2025-09-20T00:00:00+00:00</published><updated>2025-10-09T03:14:23+00:00</updated><id>/airlab/preview/pr-5/2025/09/20/computing-resources</id><content type="html" xml:base="/airlab/preview/pr-5/2025/09/20/computing-resources.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>If you follow discourse around artificial intelligence and LLMs (large language models), chances are, you’ve heard claims about their enormous resource demands (If you haven’t, see below!). Let’s take a close look at some of those claims and how we can address worries surrounding resource consumption. Since a wide variety of factors come together to create and power LLMs, we’ll limit the scope of this post to two big categories: energy and water. After some background on these inputs, we’ll explore possible avenues for reducing consumption.</p>

<h3 id="water-usage">Water Usage</h3>
<p>Often overlooked, water plays a crucial role in facilitating cloud infrastructure and thus our modern AI applications. Projections for the global water withdrawal for AI usage in 2027 range from 4.2–6.6 billion cubic meters, about half the water withdrawal of the U.K. <!-- excerpt start -->0.38–0.60 billion cubic meters of that water will be evaporated or consumed. To contextualize this measurement for a WCU student, the lower end of that range is greater than total amount of drinking water distributed in Philadelphia for a year.<!-- excerpt end --><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> Furthermore, aggressive projections for AI water withdrawal in the U.S. alone for the year 2028 exceed the 2027 global estimates<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<p><img src="/airlab/preview/pr-5/images/other/blogs/computing-resources/gpt3WaterFootprint.png" alt="GPT-3 Water Consumption Footprint" title="GPT-3 Water Consumption Footprint" /><sup id="fnref:1:2" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>
Estimate of GPT-3’s average operational water consumption footprint. ‘*’ denotes data centers under construction as of July 2023. PUE/WUE denotes Power/Water Usage Effectiveness. EWIF denotes Electricity Water Intensity Factor.</p>

<h3 id="energy-usage">Energy Usage</h3>
<p>Water and energy usage are deeply interconnected, so we’ll turn to that next. To best understand the relation between the two, let’s look at the electricity projections for the same years. Estimates for AI’s global electricity usage in 2027 range from 85–134TWh<sup id="fnref:1:3" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. To provide another local comparison, the bottom of that range is 5 times more energy than all of Philadelphia county consumed in 2024<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>. The same study used to forecast the 2028 U.S. water withdrawal figure places the country’s AI electricity consumption for the year at a range of 150–300TWh<sup id="fnref:1:4" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. To narrow the context and provide insight into a particular operation, the GPUs that trained GPT-3 used rivaled the monthly consumption of about 1,450 U.S. homes<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Regardless of whether you focus on the lower or upper bound of estimations, these figures underscore the colossal scale of what goes on behind the scenes of some of today’s most well known apps. In order to understand how things can be improved, let’s identify some of the biggest contributing factors.</p>

<h2 id="background">Background</h2>
<p>We’ve seen now how resource-intensive the modern AI ecosystem can be, but where does all the water and energy go? Considering the massive scale of AI applications, we can start at a high level and identify shortcomings with system-level infrastructure (data centers). Referring back to the chart under the “Water Usage” section, we can identify the U.S. data centers with the highest “WUE,” indicating low efficiency in on-site (cooling) water usage. When sorted by this attribute, the three least efficient data centers are the ones in Arizona (2.240), Texas (1.820), and Washington (1.090). With this grouping, it’s clear that data centers in hot, arid places require more water for cooling. This geographical placement may seem slightly perplexing; of course hotter areas are harder to cool! There are, however, other considerations when constructing new data centers. Aside from on-site water usage (and business incentives), power infrastructure also plays a role and can offer some tradeoffs with the off-site water usage. When selecting data centers by EWIF, Texas drops out of the top three, possibly due to the availability of wind and solar energy. Considering both WUE and EWIF, it becomes clear that optimizing water usage isn’t as simple as doing all computations in a cold climate.</p>

<p>Going down a level, we can consider the performance (operations per second) required to train and run sophisticated models. This power comes from GPUs. While they are performant, tasks typically do not utilize them to their fullest potential. Despite techniques to address this underutilization, across global data centers, average GPU use ranges from 30-50%<sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Another study finds an even lower rate (18%) in one production environment<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. From this insight, researchers deduce that a considerable amount of energy goes toward these processors sitting in an idle or stalled state<sup id="fnref:5:2" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<p>This underutilization draws attention to a more systemic issue: In modern processors, data must travel between a memory unit and a compute unit. Since the compute units must wait for data to arrive, a bottleneck arises based on how fast the data can be accessed (the von Neumann bottleneck)<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. While alternative architectures are being researched, we can also address this bottleneck in how we manage resources at the software level. One major component of this challenge is memory management. By optimizing memory usage, we can reduce stalls, increase throughput, and ultimately alleviate some energy demand.</p>

<h3 id="memory-organization">Memory Organization</h3>
<p>To better understand how memory management factors into performance, let’s start by taking a look at how a computer organizes data. We can divide memory into two different categories: primary (internal, volatile) and secondary (external, non-volatile). The diagram below shows the different tiers of available memory, ordered by size and access time. Registers, caches, and main memory (dynamic random-access memory) fall under the primary category. They store programs and data for the processes your computer is currently running (like this webpage!) and are directly accessible to the processor. On the other hand, things like HDDs (hard drive disks) and SSDs (solid-state drives) constitute secondary memory. They are used for long-term storage, retain data even when unpowered, and are indirectly accessible to the processor through input/output operations. As you can see in the diagram, access time for secondary storage jumps to the scale of milliseconds, whereas primary storage can be accessed in nanoseconds. Importantly, it also indicates the direction of cost per bit, a monetary measurement (primary storage is more complex to physically produce). Taking these attributes into consideration, we can see that efficient design necessitates a balance between the two categories.</p>

<p><img src="/airlab/preview/pr-5/images/other/blogs/computing-resources/Memory-Hierarchy-Design.png" alt="Memory Hierarchy Diagram" title="Memory Hierarchy Diagram" /><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup></p>

<h2 id="mitigating-resource-consumption">Mitigating Resource Consumption</h2>
<p>So far, we’ve identified a couple key factors contributing to water and energy usage in AI, namely the at-scale infrastructure supporting data centers and the underutilization of compute units in current hardware. Researchers are handling these inefficiencies at multiple levels of the technology stack. Let’s look at some examples at each level.</p>

<h3 id="hardware-advancements">Hardware Advancements</h3>
<p>Any kind of hardware used to speed up operations for AI could be classified as an AI accelerator. The most widely used accelerator is the GPUs. GPUs increase performance through parallel processing, enabled by having multiple logic cores. While performant, they were not designed specifically for AI datasets and thus leave room form improvement, as previously identified. At a high level, newer accelerators such as NPUs and TPUs (Neural/Tensor Processing Units), allow for more efficient computing by prioritizing access to primary, on-chip memory and show definite improvements compared to GPUs<sup id="fnref:5:3" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<h3 id="software-level-optimizations">Software-Level Optimizations</h3>
<p>Although hardware capabilities continue to increase, it remains important to write software that not only fully utilizes the hardware’s capabilities but also doesn’t waste those resources. There are a variety of ways in which programs handle memory inefficiently. One method called mixed-precision training cuts down on memory usage by reducing the precision of values in certain operations. This change might look like using 16-bit values instead of 32-bit ones – a 50% reduction in size! This simple adjustment, when applied strategically, dramatically increase performance with only minimal losses to accuracy<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>. Aside from reducing wastefulness, we can also more fully utilize compute units that would otherwise be idle through a method called gradient checkpointing. When a program plans to reuse previously computed data, it can save time storing only a subset of data points, which are more quickly accessible and enable the processors to redo the calculation rather than wait for the data to arrive <sup id="fnref:5:4" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<h3 id="operational-improvements">Operational Improvements</h3>
<p>As previously discussed, some data centers consume resources more efficiently than others. While future constructions may be designed to consume less resources, current builds can be used in ways that optimize their water-consumption rates. Things to consider here include how the environment’s temperature changes throughout the day or when renewable energy is most plentiful. Strategic scheduling can account for conditions like these and run when cooling and energy footprints are lower. Current usage patterns leave much room for improvement<sup id="fnref:5:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we identified the von Neumann bottleneck, a current limitation in conventional computer architecture, as well as inefficiencies in the way we use our computing resources. With such a wide array of optimizations, it’s clear that significant improvements can be made to our resource consumption. The various levels at which contributions can be made draw from fields within computer science and beyond. From the physical reactions that form the basis of computing to the governmental policies that regulate its deployment, efficient computing is a truly interdisciplinary problem that will require input from all parts of society.</p>

<h2 id="opportunities-for-future-work">Opportunities for Future Work</h2>
<p>Although this post doesn’t look deeply into any technical details, it provides a good foundation to start doing so and could be the start of a series on efficient computing. Having identified some broad areas with potential for improvements, it could be helpful to more closely examine a couple methods that were described and see how they work.</p>

<h1 id="references-and-links">References and Links</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Li, Peng et al. “Making AI Less ‘Thirsty’.” Communications of the ACM 68 (2023): 54 - 61. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:1:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Philadelphia Water Department. Resource Recovery &amp; Energy Production. Philadelphia Water Department, https://water.phila.gov/sustainability/energy/ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Shehabi, A., Smith, S.J., Horner, N., Azevedo, I., Brown, R., Koomey, J., Masanet, E., Sartor, D., Herrlin, M., Lintner, W. 2016. United States Data Center Energy Usage Report. Lawrence Berkeley National Laboratory, Berkeley, California. LBNL-1005775 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Philadelphia County, Pennsylvania Electricity Rates &amp; Statistics.” FindEnergy, FindEnergy LLC, 31 July 2025, https://findenergy.com/pa/philadelphia-county-electricity/ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Makin, Yashasvi, and Rahul Maliakkal. “Sustainable AI Training via Hardware–Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures.” <em>arXiv</em>, preprint arXiv:2508.13163, 28 July 2025, https://arxiv.org/abs/2508.13163. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:5:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:5:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:5:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Weng, Qizhen, et al. “MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters.” NSDI ’22: Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation, 4–6 Apr. 2022, Renton, WA, USENIX Association, https://www.usenix.org/system/files/nsdi22-paper-weng.pdf <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Hess, Peter. “Why a Decades Old Architecture Decision Is Impeding the Power of AI Computing.” IBM Research Blog, 10 Feb. 2025, research.ibm.com/blog/why-von-neumann-architecture-is-impeding-the-power-of-ai-computing. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Zintler, Alexander. (2022). Investigating the influence of microstructure and grain boundaries on electric properties in thin film oxide RRAM devices – A component specific approach. 10.26083/tuprints-00021657. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., &amp; Wu, H. (2018, February 15). Mixed precision training. arXiv.org. https://arxiv.org/abs/1710.03740 <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>brian-kominick</name></author><category term="ai," /><category term="energy," /><category term="resources" /><summary type="html"><![CDATA[Introduction If you follow discourse around artificial intelligence and LLMs (large language models), chances are, you’ve heard claims about their enormous resource demands (If you haven’t, see below!). Let’s take a close look at some of those claims and how we can address worries surrounding resource consumption. Since a wide variety of factors come together to create and power LLMs, we’ll limit the scope of this post to two big categories: energy and water. After some background on these inputs, we’ll explore possible avenues for reducing consumption.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/airlab/preview/pr-5/images/other/blogs/computing-resources/liquid-cooling.jpg" /><media:content medium="image" url="/airlab/preview/pr-5/images/other/blogs/computing-resources/liquid-cooling.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example post 3</title><link href="/airlab/preview/pr-5/2023/02/23/example-post-3.html" rel="alternate" type="text/html" title="Example post 3" /><published>2023-02-23T00:00:00+00:00</published><updated>2025-10-09T03:14:23+00:00</updated><id>/airlab/preview/pr-5/2023/02/23/example-post-3</id><content type="html" xml:base="/airlab/preview/pr-5/2023/02/23/example-post-3.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>john-doe</name></author><category term="biology," /><category term="medicine" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/airlab/preview/pr-5/images/photo.jpg" /><media:content medium="image" url="/airlab/preview/pr-5/images/photo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example post 2</title><link href="/airlab/preview/pr-5/2021/09/30/example-post-2.html" rel="alternate" type="text/html" title="Example post 2" /><published>2021-09-30T00:00:00+00:00</published><updated>2025-10-09T03:14:23+00:00</updated><id>/airlab/preview/pr-5/2021/09/30/example-post-2</id><content type="html" xml:base="/airlab/preview/pr-5/2021/09/30/example-post-2.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>jane-smith</name></author><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry><entry><title type="html">Example post 1</title><link href="/airlab/preview/pr-5/2019/01/07/example-post-1.html" rel="alternate" type="text/html" title="Example post 1" /><published>2019-01-07T00:00:00+00:00</published><updated>2025-10-09T03:14:23+00:00</updated><id>/airlab/preview/pr-5/2019/01/07/example-post-1</id><content type="html" xml:base="/airlab/preview/pr-5/2019/01/07/example-post-1.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>sarah-johnson</name></author><category term="biology" /><category term="medicine" /><category term="big data" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry></feed>