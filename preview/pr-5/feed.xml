<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/airlab/preview/pr-5/feed.xml" rel="self" type="application/atom+xml" /><link href="/airlab/preview/pr-5/" rel="alternate" type="text/html" /><updated>2025-10-01T15:37:58+00:00</updated><id>/airlab/preview/pr-5/feed.xml</id><title type="html">WCU-AIR</title><subtitle>The Artificial Intelligence Research (AIR) Lab at West Chester University is a collaborative community of faculty and students exploring how AI can make a positive impact. We bring together diverse computer science, engineering, and education perspectives, with a strong emphasis on mentorship and hands-on experience. Our members contribute to publications, open-source projects, and applied collaborations while developing skills that prepare them for both academic and industry careers. By fostering inclusion and curiosity, the AIR Lab provides an environment where innovation and learning thrive side by side.</subtitle><entry><title type="html">Resource Efficiency and AI</title><link href="/airlab/preview/pr-5/2025/09/20/computing-resources.html" rel="alternate" type="text/html" title="Resource Efficiency and AI" /><published>2025-09-20T00:00:00+00:00</published><updated>2025-10-01T15:34:55+00:00</updated><id>/airlab/preview/pr-5/2025/09/20/computing-resources</id><content type="html" xml:base="/airlab/preview/pr-5/2025/09/20/computing-resources.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>If you follow discourse around artificial intelligence and LLMs (large language models), chances are, you’ve heard claims about their enormous resource demands (If you haven’t, see below!). Let’s take a close look at some of those claims and how we can address worries surrounding resource consumption. Since a wide variety of factors come together to create and power LLMs, we’ll limit the scope of this post to two big categories: energy and water. After some background on these inputs, we’ll explore methods of reducing consumption, with a focus on current technical advancements in software before ending with some ideas for further contribution.</p>

<h3 id="water-usage">Water Usage</h3>
<p>Often overlooked, water plays a crucial role in facilitating cloud infrastructure and thus our modern AI applications. Projections for the global water withdrawal for AI usage in 2027 range from 4.2–6.6 billion cubic meters, about half the water withdrawal of the U.K. 0.38–0.60 billion cubic meters of that water will be evaporated or consumed<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. To contextualize this measurement for a WCU student, the lower end of that range is a higher volume than total amount of drinking water distributed in Philadelphia for a year<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Furthermore, aggressive projections for AI water withdrawal in the U.S. alone for the year 2028 exceed the 2027 global estimates<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<h3 id="energy-usage">Energy Usage</h3>
<p>Water and energy usage are deeply interconnected, so we’ll turn to that next. To best understand the relation between the two, let’s look at the electricity projections for the same years. Estimates for AI’s global electricity usage in 2027 range from 85–134TWh<sup id="fnref:1:2" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. To provide another local comparison, the bottom of that range is 5 times more energy than all of Philadelphia county consumed in 2024<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>. The same study used to forecast the 2028 U.S. water withdrawal figure places the country’s AI electricity consumption for the year at a range of 150–300TWh<sup id="fnref:1:3" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. To narrow the context and provide insight into a particular operation, the GPUs that trained GPT-3 used rivaled the monthly consumption of about 1,450 U.S. homes<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Regardless of whether you focus on the lower or upper bound of estimations, these figures underscore the colossal scale of what goes on behind the scenes of some of today’s most well known apps. Let’s see what kind of advances in software efficiency might mitigate things.</p>

<h2 id="background">Background</h2>
<p>We’ve seen now how resource-intensive the modern AI ecosystem can be, but where does all the water and energy go? To start, we should consider the performance (operations per second) required to train and run sophisticated models. This power comes from GPUs. While they are performant, tasks typically do not utilize them to their fullest potential. Despite techniques to address this underutilization, across global data centers, average GPU use ranges from 30-50%<sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Another study finds an even lower rate (18%) in one production environment<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. From this insight, researchers deduce that a considerable amount of energy goes toward these processors sitting in an idle or stalled state<sup id="fnref:5:2" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<p>This underutilization draws attention to a more systemic issue: In modern processors, data must travel between a memory unit and a compute unit. Since the compute units must wait for data to arrive, a bottleneck arises based on how fast the data can be accessed (the von Neumann bottleneck)<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. While alternative architectures are being researched, we can also address this bottleneck in how we manage resources at the software level. One major component of this challenge is memory management. By optimizing memory usage, we can reduce stalls, increase throughput, and ultimately alleviate some energy demand.</p>

<h3 id="memory-organization">Memory Organization</h3>
<p>To better understand how memory management factors into performance, let’s start by taking a look at how a computer organizes data. We can divide memory into two different categories: primary (internal, volatile) and secondary (external, non-volatile). The diagram below shows the different tiers of available memory, ordered by size and access time. Registers, caches, and main memory (dynamic random-access memory) fall under the primary category. They store programs and data for the processes your computer is currently running (like this webpage!) and are directly accessible to the processor. On the other hand, things like HDDs (hard drive disks) and SSDs (solid-state drives) constitute secondary memory. They are used for long-term storage, retain data even when unpowered, and are indirectly accessible to the processor through input/output operations. As you can see in the diagram, access time for secondary storage jumps to the scale of milliseconds, whereas primary storage can be accessed in nanoseconds. Importantly, it also indicates the direction of cost per bit, a monetary measurement (primary storage is more complex to physically produce). Taking these attributes into consideration, we can see that efficient design necessitates a balance between the two categories.</p>

<p><img src="/airlab/preview/pr-5/images/other/computing-resouces/Memory-Hierarchy-Design.png" alt="Memory Hierarchy Diagram" title="Memory Hierarchy Diagram" /><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup></p>

<h2 id="software-level-optimizations">Software-Level Optimizations</h2>
<p>To pragmatically address the bottleneck of input/output operations, let’s look at a couple specific ways we can better utilize the computing resources we have available: kernel fusion and gradient checkpointing<sup id="fnref:5:3" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<h3 id="kernel-fusion">Kernel Fusion</h3>
<p>Since i/o operations to secondary memory are more computationally expensive, we can try to reduce them by maximizing the potential of our primary memory through kernel fusion. This technique combines multiple operations instead of executing them sequentially and reading/writing temporary values to memory after each one.</p>

<h4 id="user-driven-example">User-Driven Example</h4>
<p>To simplify this process, consider the following conceptual example:</p>

<p>$$(5 + 3) * 10$$</p>

<p>To evaluate this expression, we could do the following</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add_and_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">temp</span> <span class="o">*</span> <span class="n">c</span>
<span class="k">print</span><span class="p">(</span><span class="n">add_and_multiply</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p>In this example, we create a temporary variable to represent the intermediary result. Instead of allocating this memory to add and multiply separately, we can combine them into a single operation, removing the need to allocate memory and read/write to it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add_and_multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
<span class="k">print</span><span class="p">(</span><span class="n">add_and_multiply</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p>Comparing these two functions is a very simple way of illustrating what happens when kernels are fused in machine-learning tasks. While this example is small, the classic example for kernel fusion involves linear algebra and sequential matrix operations. Data for these operations can measure in gigabytes and utilize secondary memory.</p>

<h4 id="compiler-driven-example">Compiler-Driven Example</h4>
<p>While a developer can manually combine operations, this strategy is largely applied at a compiler level. Instead of applying optimizations directly to source code, IRs (intermediate representations) enable the transformation of high-level languages into a lower, hardware-agnostic form, while still retaining important semantic information from the original form. MLIR (Multi-Level Intermediate Representation) is a specific form or framework which makes use of fusion and is widely used for machine learning. Like the name suggests, MLIR allows code to be represented at varying levels of abstraction, each conducive to different optimizations<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</p>

<p>Using MLIR, compilers can view code as a DAG (directed acyclic graph) which shows data dependencies within the program. With nodes in the graph representing operators, it reveals the flow of data from one operator to the next. When an operator immediately consumes the output from the one preceding it, it could be advantageous to fuse the operations rather than storing and subsequently collecting the data. Let’s take a look at a simple arithmetic example. Consider the expression:</p>

<p>$$a + b * c + 1$$</p>

<p>To keep things more accessible, let’s implement our own tiny IR just for this example. The signatures could look like this:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">Node</span> <span class="p">{</span>
    <span class="kr">inline</span> <span class="k">static</span> <span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span><span class="p">;</span>
    <span class="n">Node</span><span class="p">()</span> <span class="o">:</span> <span class="n">id</span><span class="p">(</span><span class="n">counter</span><span class="o">++</span><span class="p">)</span> <span class="p">{}</span>
    <span class="k">virtual</span> <span class="o">~</span><span class="n">Node</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarA</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarA</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarB</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarB</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">VarC</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">VarC</span><span class="p">();</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Const</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">value</span><span class="p">;</span>
    <span class="n">Const</span><span class="p">(</span><span class="kt">double</span> <span class="n">v</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Add</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">;</span>
    <span class="n">Add</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">l</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">Mult</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">;</span>
    <span class="n">Mult</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">l</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">);</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">FusedOp</span> <span class="o">:</span> <span class="n">Node</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">;</span>
    <span class="k">explicit</span> <span class="n">FusedOp</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&gt;</span> <span class="n">in</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div></div>

<p>If we parse this expression into a DAG using our minimal IR, we could represent it like this:</p>

<p><img src="/airlab/preview/pr-5/images/other/computing-resouces/before.png" alt="Before Fusion" title="Before Fusion" /></p>

<p>From here, we can see the relationships between each term and apply a function to recursively traverse the tree. That could transform the original graph into the following:</p>

<p><img src="/airlab/preview/pr-5/images/other/computing-resouces/after.png" alt="After Fusion" title="After Fusion" /></p>

<p>Implementing this practice in a compiler removes the need to manually apply it to all operations and allows for further optimizations at a smaller level.</p>

<p>The complete code for this implementation can be found <a href="https://github.com/brankominick/fusion-demo">here</a>.</p>

<h3 id="gradient-checkpointing">Gradient Checkpointing</h3>
<p>Since bottlenecks are often caused by memory operations and leave computational units idle, we can use the gradient checkpointing strategy to reduce the amount of memory we use in return for increasing the amount of computations being done. In other words, instead of recording all of the data you need, record only a subset (checkpoints) and use it to redo the rest of the calculations as needed. It’s like the reverse of memoization. Unlike kernel fusion, this method is more limited in its use cases but very applicable to the backwards passes in machine learning and AI training. Because it’s more specific to AI training, it can be found implemented at the framework level like in PyTorch<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</p>

<p>In order to do a short demonstration without abstracting away the logic, let’s put together a simple implementation and take some benchmarks. To account for storage, we compare saving each intermediate value with saving only some:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span> <span class="nf">compute_with_storage</span><span class="p">(</span><span class="kt">double</span> <span class="n">x</span><span class="p">,</span> <span class="kt">int</span> <span class="n">depth</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">intermediates</span><span class="p">(</span><span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">intermediates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">depth</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"  Storage used: "</span>
              <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">fixed</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">setprecision</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
              <span class="o">&lt;&lt;</span> <span class="n">memory_in_mb</span><span class="p">(</span><span class="n">intermediates</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
              <span class="o">&lt;&lt;</span> <span class="s">" MB</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">intermediates</span><span class="p">[</span><span class="n">depth</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To cut down on intermediates, instead of recording each one, we can update a variable and only record the checkpoints:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">depth</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">double</span> <span class="n">val</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">.</span><span class="n">back</span><span class="p">();</span>

        <span class="kt">int</span> <span class="n">steps</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">checkpoint_interval</span><span class="p">,</span> <span class="n">depth</span> <span class="o">-</span> <span class="n">i</span><span class="p">);</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">steps</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">val</span> <span class="o">=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">val</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">checkpoints</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">steps</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>With an x value of 1.001 and a ridiculous depth of 2 billion, we get the following results:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Storage used: 15258.79 MB
Execution <span class="nb">time </span>with storage: 28.81s

K,Memory_MB,Time_s
10,1525.88,7.10
50,305.18,6.24
100,152.59,6.11
500,30.52,6.09
1000,15.26,6.10
5000,3.05,6.07
10000,1.53,6.05
50000,0.31,6.03
</code></pre></div></div>

<script src="https://d3js.org/d3.v7.min.js"></script>

<div id="chart"></div>
<div id="info-box" style="margin-top:20px; font-family:sans-serif;">
  <p>Hover over a point to see details.</p>
</div>
<script src="/airlab/preview/pr-5/assets/js/checkpoint-plot.js"></script>

<p>As we can see, adding checkpoints dramatically reduces not only the amount of memory used but also the total run time. However, the checkpoints have diminishing returns. As they become more frequent (as K decreases), the memory usage and execution time both increase. If we continue to decrease checkpoints (increase K), we would see slight increases to execution time in that direction as well.</p>

<p>The complete code for this implementation can be found <a href="https://github.com/brankominick/gradient-checkpointing-demo">here</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this post, we identified the von Neumann bottleneck, a current limitation in conventional computer architecture, as well as a couple programming strategies to mitigate its impact. While each of these strategies brings its own optimizations, they also come with their own limitations, requiring careful consideration in how they’re implemented. Additionally, it’s important to remember that savings will compound when these techniques and others are applied in conjunction, resulting in higher utilization of compute units and thus, less wasted energy. Aside from software-level optimizations, researchers continue to bring advancements to hardware capabilities and operational procedures<sup id="fnref:5:4" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<h2 id="opportunities-for-future-work">Opportunities for Future Work</h2>
<p>As noted in the conclusion, the strategies covered in this post have their limitations. Future work could include examining these bounds such as the payoff for fusion with complex kernels or how preferring primary memory may limit high-precision values. We could also explore other software-level optimization methods. Additionally, we could go in a more concrete direction and extend current frameworks like MLIR to execute and benchmark some theoretical or unproven methods. Follow along to see what comes next!</p>

<h1 id="references-and-links">References and Links</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Li, Peng et al. “Making AI Less ‘Thirsty’.” Communications of the ACM 68 (2023): 54 - 61. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Philadelphia Water Department. Resource Recovery &amp; Energy Production. Philadelphia Water Department, https://water.phila.gov/sustainability/energy/ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Shehabi, A., Smith, S.J., Horner, N., Azevedo, I., Brown, R., Koomey, J., Masanet, E., Sartor, D., Herrlin, M., Lintner, W. 2016. United States Data Center Energy Usage Report. Lawrence Berkeley National Laboratory, Berkeley, California. LBNL-1005775 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Philadelphia County, Pennsylvania Electricity Rates &amp; Statistics.” FindEnergy, FindEnergy LLC, 31 July 2025, https://findenergy.com/pa/philadelphia-county-electricity/ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Makin, Yashasvi, and Rahul Maliakkal. “Sustainable AI Training via Hardware–Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures.” <em>arXiv</em>, preprint arXiv:2508.13163, 28 July 2025, https://arxiv.org/abs/2508.13163. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:5:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:5:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Weng, Qizhen, et al. “MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters.” NSDI ’22: Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation, 4–6 Apr. 2022, Renton, WA, USENIX Association, https://www.usenix.org/system/files/nsdi22-paper-weng.pdf <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Hess, Peter. “Why a Decades Old Architecture Decision Is Impeding the Power of AI Computing.” IBM Research Blog, 10 Feb. 2025, research.ibm.com/blog/why-von-neumann-architecture-is-impeding-the-power-of-ai-computing. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Zintler, Alexander. (2022). Investigating the influence of microstructure and grain boundaries on electric properties in thin film oxide RRAM devices – A component specific approach. 10.26083/tuprints-00021657. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. “MLIR: Scaling compiler infrastructure for domain specific computation.” In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 2-14. IEEE, 2021. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Paszke, Adam, et al. Automatic Differentiation in PyTorch. 2017, OpenReview, https://openreview.net/pdf?id=BJJsrmfCZ <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>brian-kominick</name></author><category term="ai," /><category term="energy," /><category term="resources" /><summary type="html"><![CDATA[Introduction If you follow discourse around artificial intelligence and LLMs (large language models), chances are, you’ve heard claims about their enormous resource demands (If you haven’t, see below!). Let’s take a close look at some of those claims and how we can address worries surrounding resource consumption. Since a wide variety of factors come together to create and power LLMs, we’ll limit the scope of this post to two big categories: energy and water. After some background on these inputs, we’ll explore methods of reducing consumption, with a focus on current technical advancements in software before ending with some ideas for further contribution.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/airlab/preview/pr-5/images/photo.jpg" /><media:content medium="image" url="/airlab/preview/pr-5/images/photo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example post 3</title><link href="/airlab/preview/pr-5/2023/02/23/example-post-3.html" rel="alternate" type="text/html" title="Example post 3" /><published>2023-02-23T00:00:00+00:00</published><updated>2025-10-01T15:34:55+00:00</updated><id>/airlab/preview/pr-5/2023/02/23/example-post-3</id><content type="html" xml:base="/airlab/preview/pr-5/2023/02/23/example-post-3.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>john-doe</name></author><category term="biology," /><category term="medicine" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/airlab/preview/pr-5/images/photo.jpg" /><media:content medium="image" url="/airlab/preview/pr-5/images/photo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example post 2</title><link href="/airlab/preview/pr-5/2021/09/30/example-post-2.html" rel="alternate" type="text/html" title="Example post 2" /><published>2021-09-30T00:00:00+00:00</published><updated>2025-10-01T15:34:55+00:00</updated><id>/airlab/preview/pr-5/2021/09/30/example-post-2</id><content type="html" xml:base="/airlab/preview/pr-5/2021/09/30/example-post-2.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>jane-smith</name></author><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry><entry><title type="html">Example post 1</title><link href="/airlab/preview/pr-5/2019/01/07/example-post-1.html" rel="alternate" type="text/html" title="Example post 1" /><published>2019-01-07T00:00:00+00:00</published><updated>2025-10-01T15:34:55+00:00</updated><id>/airlab/preview/pr-5/2019/01/07/example-post-1</id><content type="html" xml:base="/airlab/preview/pr-5/2019/01/07/example-post-1.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>sarah-johnson</name></author><category term="biology" /><category term="medicine" /><category term="big data" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry></feed>